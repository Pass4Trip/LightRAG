{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!huggingface-cli login --token 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\n!pip install pika\n!pip install lightrag-hku==1.0.0 aioboto3 ollama openai tiktoken nano_vectordb neo4j einops\n!pip install nest_asyncio\n!pip install -U transformers autoawq accelerate bitsandbytes\n!pip install -U jupyterlab_widgets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:49.479934Z","iopub.execute_input":"2024-11-26T12:40:49.480674Z","iopub.status.idle":"2024-11-26T12:42:04.589610Z","shell.execute_reply.started":"2024-11-26T12:40:49.480626Z","shell.execute_reply":"2024-11-26T12:42:04.588734Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nCollecting pika\n  Downloading pika-1.3.2-py3-none-any.whl.metadata (13 kB)\nDownloading pika-1.3.2-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pika\nSuccessfully installed pika-1.3.2\nCollecting lightrag-hku==1.0.0\n  Downloading lightrag_hku-1.0.0-py3-none-any.whl.metadata (34 kB)\nCollecting aioboto3\n  Downloading aioboto3-13.2.0-py3-none-any.whl.metadata (8.8 kB)\nCollecting ollama\n  Downloading ollama-0.4.1-py3-none-any.whl.metadata (4.8 kB)\nCollecting openai\n  Downloading openai-1.55.1-py3-none-any.whl.metadata (24 kB)\nCollecting tiktoken\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting nano_vectordb\n  Downloading nano_vectordb-0.0.4.3-py3-none-any.whl.metadata (3.7 kB)\nCollecting neo4j\n  Downloading neo4j-5.26.0-py3-none-any.whl.metadata (5.9 kB)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting aiobotocore==2.15.2 (from aiobotocore[boto3]==2.15.2->aioboto3)\n  Downloading aiobotocore-2.15.2-py3-none-any.whl.metadata (23 kB)\nCollecting aiofiles>=23.2.1 (from aioboto3)\n  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: botocore<1.35.37,>=1.35.16 in /opt/conda/lib/python3.10/site-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (1.35.23)\nRequirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (3.9.5)\nRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (1.16.0)\nRequirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (0.12.0)\nCollecting boto3<1.35.37,>=1.35.16 (from aiobotocore[boto3]==2.15.2->aioboto3)\n  Downloading boto3-1.35.36-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from ollama) (0.27.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from ollama) (2.9.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nano_vectordb) (1.26.4)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from neo4j) (2024.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (4.0.3)\nCollecting botocore<1.35.37,>=1.35.16 (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3)\n  Downloading botocore-1.35.36-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<1.35.37,>=1.35.16->aiobotocore[boto3]==2.15.2->aioboto3) (1.0.1)\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3<1.35.37,>=1.35.16->aiobotocore[boto3]==2.15.2->aioboto3)\n  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.37,>=1.35.16->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.37,>=1.35.16->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3) (1.16.0)\nDownloading lightrag_hku-1.0.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aioboto3-13.2.0-py3-none-any.whl (34 kB)\nDownloading aiobotocore-2.15.2-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ollama-0.4.1-py3-none-any.whl (12 kB)\nDownloading openai-1.55.1-py3-none-any.whl (389 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.5/389.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nano_vectordb-0.0.4.3-py3-none-any.whl (5.6 kB)\nDownloading neo4j-5.26.0-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\nDownloading jiter-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading boto3-1.35.36-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.35.36-py3-none-any.whl (12.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: neo4j, nano_vectordb, lightrag-hku, jiter, einops, aiofiles, tiktoken, botocore, s3transfer, openai, ollama, aiobotocore, boto3, aioboto3\n  Attempting uninstall: aiofiles\n    Found existing installation: aiofiles 22.1.0\n    Uninstalling aiofiles-22.1.0:\n      Successfully uninstalled aiofiles-22.1.0\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.35.23\n    Uninstalling botocore-1.35.23:\n      Successfully uninstalled botocore-1.35.23\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.6.2\n    Uninstalling s3transfer-0.6.2:\n      Successfully uninstalled s3transfer-0.6.2\n  Attempting uninstall: aiobotocore\n    Found existing installation: aiobotocore 2.15.1\n    Uninstalling aiobotocore-2.15.1:\n      Successfully uninstalled aiobotocore-2.15.1\n  Attempting uninstall: boto3\n    Found existing installation: boto3 1.26.100\n    Uninstalling boto3-1.26.100:\n      Successfully uninstalled boto3-1.26.100\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aioboto3-13.2.0 aiobotocore-2.15.2 aiofiles-24.1.0 boto3-1.35.36 botocore-1.35.36 einops-0.8.0 jiter-0.7.1 lightrag-hku-1.0.0 nano_vectordb-0.0.4.3 neo4j-5.26.0 ollama-0.4.1 openai-1.55.1 s3transfer-0.10.4 tiktoken-0.8.0\nRequirement already satisfied: nest_asyncio in /opt/conda/lib/python3.10/site-packages (1.6.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting autoawq\n  Downloading autoawq-0.2.7.post2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: torch>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from autoawq) (2.4.0)\nCollecting triton (from autoawq)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from autoawq) (4.12.2)\nRequirement already satisfied: datasets>=2.20 in /opt/conda/lib/python3.10/site-packages (from autoawq) (3.0.1)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq) (0.23.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.20->autoawq) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq) (3.9.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.2.0->autoawq) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.2.0->autoawq) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.16.0)\nDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading autoawq-0.2.7.post2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, bitsandbytes, accelerate, transformers, autoawq\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed accelerate-1.1.1 autoawq-0.2.7.post2 bitsandbytes-0.44.1 transformers-4.46.3 triton-3.1.0\nRequirement already satisfied: jupyterlab_widgets in /opt/conda/lib/python3.10/site-packages (3.0.11)\nCollecting jupyterlab_widgets\n  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\nDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jupyterlab_widgets\n  Attempting uninstall: jupyterlab_widgets\n    Found existing installation: jupyterlab_widgets 3.0.11\n    Uninstalling jupyterlab_widgets-3.0.11:\n      Successfully uninstalled jupyterlab_widgets-3.0.11\nSuccessfully installed jupyterlab_widgets-3.0.13\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pika\nimport os\n\nclass RabbitMQ:\n    def __init__(self):\n        self.user = os.getenv('RABBITMQ_USER', 'rabbitmq')\n        self.password = os.getenv('RABBITMQ_PASSWORD', 'mypassword')\n        self.host = os.getenv('RABBITMQ_HOST', 'vps-af24e24d.vps.ovh.net')\n        self.port = int(os.getenv('RABBITMQ_PORT', 30645))\n        self.connection = None\n        self.channel = None\n        self.connect()\n\n    def connect(self):\n        credentials = pika.PlainCredentials(self.user, self.password)\n        parameters = pika.ConnectionParameters(host=self.host, port=self.port, credentials=credentials)\n        self.connection = pika.BlockingConnection(parameters)\n        self.channel = self.connection.channel()\n\n    def close(self):\n        if self.connection and not self.connection.is_closed:\n            self.connection.close()\n\n    def consume(self, queue_name, callback):\n        if not self.channel:\n            raise Exception(\"Connection is not established.\")\n        self.channel.basic_consume(queue=queue_name, on_message_callback=callback, auto_ack=False)\n        self.channel.start_consuming()\n\n    def publish(self, queue_name, message):\n        if not self.channel:\n            raise Exception(\"Connection is not established.\")\n        self.channel.queue_declare(queue=queue_name, durable=True)\n        self.channel.basic_publish(exchange='',\n                                   routing_key=queue_name,\n                                   body=message,\n                                   properties=pika.BasicProperties(\n                                       delivery_mode=2,  # make message persistent\n                                   ))\n        print(f\"Sent message to queue {queue_name}: {message}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:42:04.591479Z","iopub.execute_input":"2024-11-26T12:42:04.591765Z","iopub.status.idle":"2024-11-26T12:42:04.619498Z","shell.execute_reply.started":"2024-11-26T12:42:04.591733Z","shell.execute_reply":"2024-11-26T12:42:04.618821Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from lightrag import LightRAG\nfrom lightrag.llm import hf_model_complete, hf_embedding, ollama_model_complete, ollama_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.prompt import PROMPTS\nfrom transformers import AutoModel, AutoTokenizer\nimport asyncio\nimport os\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nWORKING_DIR = \"/kaggle/working/\"\n\nos.environ[\"NEO4J_URI\"] = \"neo4j://vps-af24e24d.vps.ovh.net:32719\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"my-initial-password\"\n\nPROMPTS[\"DEFAULT_ENTITY_TYPES\"] = [\"restaurant\", \"resume\", \"positive_point\", \"negative_point\", \"recommandation\"]\n\nPROMPTS[\"entity_extraction\"] = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Extract the main entity, in this is the restaurant, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [{entity_types}]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>\n\n1. Identify all entities realted to the entity in step 1. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [{entity_types}]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>\nAnd create is relationship of the following format (restaurant_entity, entity).\nFor each relationship, extract the following information:\n- restaurant_entity: name of the restaurant entity, as identified in step 1\n- entity: name of the target entity you identified\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details\nFormat each relationship as (\"relationship\"{tuple_delimiter}<restaurant_entity>{tuple_delimiter}<entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)\nAll entities need to have a relationship, if an entity doesn't have a relationship don't add it or create a relationship \n\n2. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.\nFormat the content-level key words as (\"content_keywords\"{tuple_delimiter}<high_level_keywords>)\n\n3. Ensure that every identified entity must have at least one relationship explicitly linking it to the 'restaurant' entity.\nAll entities in step 1 must be connected to the entity type \"RESTAURANT\"\nYou must add relationship with the following format to complete all relationships: (\"relationship\"{tuple_delimiter}<restaurant_entity>{tuple_delimiter}<entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)\nYou need to have the same number of entities in relationship minus one, for example: if you have 14 entities you need 13 relationships, other example if you have 16 entities you need 15 relationships.\n\n4. For **positive_point**, **negative_point**, and **recommandation**, generate generic and concise labels that summarize key aspects applicable across different restaurants.\n\n5. Return output in French as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.\n\n6. When finished, output {completion_delimiter}\n\n-Examples-\nExample 1:\n\nEntity_types: [restaurant, positive_point, negative_point, recommandation]\nText: \nRésumé du restaurant Le Coquemar:  Le Coquemar est un restaurant français qui offre une expérience culinaire traditionnelle dans un cadre élégant et lumineux. Situé dans un lieu où les murs de pierres sont décorés de peintures, il propose une ambiance chaleureuse et décontractée, parfaite pour les repas en famille ou en groupe.  Le prix de la nourriture se situe entre 20 et 30 euros, offrant un bon rapport qualité-prix pour les clients. Les avis clients sont généralement positifs, avec des notes allant de 4 à 5 étoiles pour la nourriture, le service et l'atmosphère. Les clients apprécient particulièrement la cuisine traditionnelle, la qualité des plats et le service attentif.  Le Coquemar est également connu pour ses options de boissons, incluant des alcools, des bières, des cafés, des cocktails et des apéritifs. Les clients peuvent également choisir des produits sains, des spiritueux, des vins et des desserts. Le restaurant propose également des traiteurs et des cartes de crédit pour faciliter les paiements.  En termes de critiques, les clients ont souvent mentionné la bonne cuisine traditionnelle et le bon service. Cependant, certains critiques ont suggéré une amélioration de l'atmosphère, qui a été notée à 2 étoiles dans certains cas.  Enfin, Le Coquemar propose des repas sur place, des réservations acceptées, des options de paiement telles que les cartes de crédit, les cartes de paiement, les chèques, les paiements mobiles NFC et Pluxee. Le restaurant est également chaleureux et accueillant, avec des toilettes disponibles et des titres restaurant pour les clients.\nOutput:\n(\"entity\"{tuple_delimiter}\"Le Coquemar\"{tuple_delimiter}\"restaurant\"{tuple_delimiter}\"Le Coquemar est un restaurant français qui offre une expérience culinaire traditionnelle dans un cadre élégant et lumineux.\"){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"ambiance chaleureuse\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Le restaurant offre une ambiance chaleureuse et décontractée.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Le Coquemar\"{tuple_delimiter}\"ambiance chaleureuse\"{tuple_delimiter}\"Le restaurant propose une ambiance chaleureuse.\"{tuple_delimiter}\"ambiance\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"cadre attrayant\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Le restaurant dispose d'un cadre élégant et lumineux.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Le Coquemar\"{tuple_delimiter}\"cadre attrayant\"{tuple_delimiter}\"Le restaurant dispose d'un cadre élégant et lumineux.\"{tuple_delimiter}\"cadre attrayant\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"service de qualité\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Les clients apprécient le service attentif.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Le Coquemar\"{tuple_delimiter}\"service de qualité\"{tuple_delimiter}\"Le service est attentif et apprécié des clients.\"{tuple_delimiter}\"service de qualité\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"amélioration de l'atmosphère\"{tuple_delimiter}\"negative_point\"{tuple_delimiter}\"Certains clients suggèrent une amélioration de l'atmosphère.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Le Coquemar\"{tuple_delimiter}\"amélioration de l'atmosphère\"{tuple_delimiter}\"Certains clients suggèrent une amélioration de l'atmosphère.\"{tuple_delimiter}\"amélioration nécessaire\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"cuisine recommandée\"{tuple_delimiter}\"recommandation\"{tuple_delimiter}\"Les clients recommandent la cuisine traditionnelle.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Le Coquemar\"{tuple_delimiter}\"cuisine recommandée\"{tuple_delimiter}\"La cuisine traditionnelle est particulièrement appréciée des clients.\"{tuple_delimiter}\"cuisine recommandée\"{tuple_delimiter}1){record_delimiter}\n\n\nExample 2:\n\nEntity_types: [restaurant, positive_point, negative_point, recommandation]\nText:\nRésumé du Café Lisboa:  Le Café Lisboa est un restaurant décontracté situé dans un cadre coloré, offrant une expérience culinaire portugaise avec une terrasse pour profiter de la vue et des tartes à la crème portugaises. Le prix de la nourriture se situe entre 20 et 30 euros, ce qui correspond à un rapport qualité-prix raisonnable.   L'ambiance du lieu est chaleureuse et conviviale, avec une terrasse accessible aux personnes à mobilité réduite. Le restaurant est également adapté aux familles et aux groupes, avec des options de réservation disponibles.   Les avis clients sont partagés, certains appréciant la cuisine authentique et les plats copieux, tandis que d'autres expriment leur déception concernant le fait que le restaurant ne propose pas de café. Les critiques récurrentes soulignent l'importance de faire une réservation pour éviter les longues files d'attente.   Les points forts du Café Lisboa incluent la cuisine portugaise, les spécialités comme le chorizo flambé et les croquettes de morue, ainsi que les excellents cocktails et les produits sains proposés. Cependant, il est recommandé de ne pas attendre pour commander, car le service peut être lent.   En conclusion, le Café Lisboa offre une expérience culinaire portugaise décontractée avec des plats frais et des cocktails raffinés. Bien que certains aspects du service puissent être améliorés, le restaurant est une option appréciée pour un dîner casual dans le centre ville de Lyon.\n\nOutput:\n(\"entity\"{tuple_delimiter}\"Café Lisboa\"{tuple_delimiter}\"restaurant\"{tuple_delimiter}\"Le Café Lisboa est un restaurant décontracté situé dans un cadre coloré, offrant une expérience culinaire portugaise avec une terrasse.\"){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"ambiance conviviale\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Le restaurant offre une ambiance chaleureuse et conviviale.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Café Lisboa\"{tuple_delimiter}\"ambiance conviviale\"{tuple_delimiter}\"Le restaurant offre une ambiance chaleureuse et conviviale.\"{tuple_delimiter}\"ambiance\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"cadre attrayant\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Le restaurant est situé dans un cadre coloré.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Café Lisboa\"{tuple_delimiter}\"cadre attrayant\"{tuple_delimiter}\"Le restaurant est situé dans un cadre coloré.\"{tuple_delimiter}\"cadre\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"adapté aux familles\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Le restaurant est adapté aux familles et aux groupes.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Café Lisboa\"{tuple_delimiter}\"adapté aux familles\"{tuple_delimiter}\"Le restaurant est adapté aux familles et aux groupes.\"{tuple_delimiter}\"adapté aux familles\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"service lent\"{tuple_delimiter}\"negative_point\"{tuple_delimiter}\"Le service peut être lent, il est donc recommandé de commander rapidement.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Café Lisboa\"{tuple_delimiter}\"service lent\"{tuple_delimiter}\"Le service peut être lent, ce qui nécessite de commander rapidement.\"{tuple_delimiter}\"service\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"cuisine portugaise\"{tuple_delimiter}\"recommandation\"{tuple_delimiter}\"La cuisine portugaise et les spécialités comme le chorizo flambé sont recommandées.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"Café Lisboa\"{tuple_delimiter}\"cuisine portugaise\"{tuple_delimiter}\"Les spécialités comme le chorizo flambé sont particulièrement appréciées.\"{tuple_delimiter}\"spécialités culinaires\"{tuple_delimiter}\n\n\nExample 3:\n\nEntity_types: [restaurant, positive_point, negative_point, recommandation]\nText:\nRésumé de ELLA Bolerie Méditerranéenne - Sainte-Foy-Lès-Lyon   ELLA Bolerie Méditerranéenne est un restaurant rapide situé dans la ville de Sainte-Foy-Lès-Lyon, offrant une expérience culinaire rapide et conviviale. Bien que le restaurant soit spécialisé dans la restauration rapide, il se distingue par sa spécialité méditerranéenne, proposant une variété de plats savoureux et rapidement préparés.   L'ambiance du restaurant est accueillante et moderne, avec un design qui favorise une atmosphère chaleureuse et conviviale. Le service est rapide et efficace, ce qui est essentiel dans un établissement de restauration rapide.   En ce qui concerne le rapport qualité-prix, le restaurant offre une gamme de prix qui est compétitive par rapport à d'autres options de restauration rapide dans la région. Les plats sont généralement bien reçus par les clients, bien que le restaurant n'ait pas reçu de notes étoilées.   Les points forts du restaurant incluent sa capacité à servir des repas rapidement sans compromettre la qualité, ainsi que sa disponibilité pour les livraisons sans contact et le service de drive. ELLA Bolerie Méditerranéenne est également adapté aux familles et aux enfants, avec des options de menu pour les convives végétariens.   Les critiques récurrentes suggèrent que le restaurant pourrait bénéficier d'une meilleure notoriété et d'une amélioration de la qualité des plats pour obtenir des étoiles. De plus, il est recommandé d'offrir une plus grande variété de plats pour attirer une clientèle plus large.   Enfin, le restaurant est ouvert aux heures habituelles d'une restauration rapide, propose un parking accessible en fauteuil roulant, et dispose d'un bar à salade et de cafés pour les clients qui souhaitent prendre un café ou un encas. La livraison et le service de drive sont également disponibles, ce qui le rend accessible pour les clients qui ne peuvent pas se rendre sur place.\n\nOutput:\n(\"entity\"{tuple_delimiter}\"ELLA Bolerie Méditerranéenne - Sainte-Foy-Lès-Lyon\"{tuple_delimiter}\"restaurant\"{tuple_delimiter}\"ELLA Bolerie Méditerranéenne est un restaurant rapide situé dans la ville de Sainte-Foy-Lès-Lyon, offrant une expérience culinaire rapide et conviviale.\"){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"service rapide\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Le service est rapide et efficace.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"ELLA Bolerie Méditerranéenne - Sainte-Foy-Lès-Lyon\"{tuple_delimiter}\"service rapide\"{tuple_delimiter}\"Le service est rapide et efficace.\"{tuple_delimiter}\"service\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"ambiance moderne\"{tuple_delimiter}\"positive_point\"{tuple_delimiter}\"Le restaurant a une ambiance moderne et conviviale.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"ELLA Bolerie Méditerranéenne - Sainte-Foy-Lès-Lyon\"{tuple_delimiter}\"ambiance moderne\"{tuple_delimiter}\"Le restaurant a une ambiance moderne et accueillante.\"{tuple_delimiter}\"ambiance\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"menu varié\"{tuple_delimiter}\"recommandation\"{tuple_delimiter}\"Il est recommandé d'offrir une plus grande variété de plats pour attirer plus de clients.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"ELLA Bolerie Méditerranéenne - Sainte-Foy-Lès-Lyon\"{tuple_delimiter}\"menu varié\"{tuple_delimiter}\"Il est recommandé d'offrir une plus grande variété de plats.\"{tuple_delimiter}\"variété\"{tuple_delimiter}1){record_delimiter}\n\n(\"entity\"{tuple_delimiter}\"amélioration de la qualité\"{tuple_delimiter}\"negative_point\"{tuple_delimiter}\"Le restaurant pourrait améliorer la qualité des plats.\"){record_delimiter}\n(\"relationship\"{tuple_delimiter}\"ELLA Bolerie Méditerranéenne - Sainte-Foy-Lès-Lyon\"{tuple_delimiter}\"amélioration de la qualité\"{tuple_delimiter}\"Le restaurant pourrait améliorer la qualité des plats pour satisfaire les clients.\"{tuple_delimiter}\"amélioration qualité\"{tuple_delimiter}1){record_delimiter}\n<|eot_id|><|start_header_id|>user<|end_header_id|>\nEntity_types: {entity_types}\nText: {input_text}\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nPROMPTS[\n    \"summarize_entity_descriptions\"\n] = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\nEntities: {entity_name}\nDescription List: {description_list}\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nPROMPTS[\n    \"entiti_continue_extraction\"\n] = \"\"\"MANY entities were missed in the last extraction.  Add them below using the same format:\n\"\"\"\n\nPROMPTS[\n    \"entiti_if_loop_extraction\"\n] = \"\"\"It appears some entities may have still been missed.  Answer YES | NO if there are still entities that need to be added.\n\"\"\"\n\nPROMPTS[\"fail_response\"] = \"Sorry, I'm not able to provide an answer to that question.\"\n\nPROMPTS[\"rag_response\"] = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n---Role---\n\nYou are a helpful assistant responding to questions about data in the tables provided.\n\n\n---Goal---\n\nGenerate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\nIf you don't know the answer, just say so. Do not make anything up.\nDo not include information where the supporting evidence for it is not provided.\n\n---Target response length and format---\n\n{response_type}\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n---Data tables---\n\n{context_data}\n\nAdd sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nPROMPTS[\"keywords_extraction\"] = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n---Role---\n\nYou are a helpful assistant tasked with identifying both high-level and low-level keywords in the user's query.\n\n---Goal---\n\nGiven the query, list both high-level and low-level keywords. High-level keywords focus on overarching concepts or themes, while low-level keywords focus on specific entities, details, or concrete terms.\n\n---Instructions---\n\n- Output the keywords in JSON format.\n- The JSON should have two keys:\n  - \"high_level_keywords\" for overarching concepts or themes.\n  - \"low_level_keywords\" for specific entities or details.\n\n-Examples-\n\nExample 1:\n\nQuery: \"How does international trade influence global economic stability?\"\n\nOutput:\n{{\n  \"high_level_keywords\": [\"International trade\", \"Global economic stability\", \"Economic impact\"],\n  \"low_level_keywords\": [\"Trade agreements\", \"Tariffs\", \"Currency exchange\", \"Imports\", \"Exports\"]\n}}\n\nExample 2:\n\nQuery: \"What are the environmental consequences of deforestation on biodiversity?\"\n\nOutput:\n{{\n  \"high_level_keywords\": [\"Environmental consequences\", \"Deforestation\", \"Biodiversity loss\"],\n  \"low_level_keywords\": [\"Species extinction\", \"Habitat destruction\", \"Carbon emissions\", \"Rainforest\", \"Ecosystem\"]\n}}\n\nExample 3:\n\nQuery: \"What is the role of education in reducing poverty?\"\n\nOutput:\n{{\n  \"high_level_keywords\": [\"Education\", \"Poverty reduction\", \"Socioeconomic development\"],\n  \"low_level_keywords\": [\"School access\", \"Literacy rates\", \"Job training\", \"Income inequality\"]\n}}\n<|eot_id|><|start_header_id|>user<|end_header_id|>\nQuery: {query}\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nPROMPTS[\"naive_rag_response\"] = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n---Role---\n\nYou are a helpful assistant responding to questions about documents provided.\n\n\n---Goal---\n\nGenerate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\nIf you don't know the answer, just say so. Do not make anything up.\nDo not include information where the supporting evidence for it is not provided.\n\n---Target response length and format---\n\n{response_type}\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n---Documents---\n\n{content_data}\n\nAdd sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:42:04.620636Z","iopub.execute_input":"2024-11-26T12:42:04.620881Z","iopub.status.idle":"2024-11-26T12:42:10.330585Z","shell.execute_reply.started":"2024-11-26T12:42:04.620856Z","shell.execute_reply":"2024-11-26T12:42:10.329880Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"rag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=hf_model_complete,\n    llm_model_name=\"hugging-quants/Meta-Llama-3.1-8B-Instruct-BNB-NF4\",\n    kg=\"Neo4JStorage\",\n    log_level=\"INFO\",\n    llm_model_max_token_size=100000,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=100000,\n        func=lambda texts: hf_embedding(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n            embed_model=AutoModel.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n        ),\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:42:10.332478Z","iopub.execute_input":"2024-11-26T12:42:10.333387Z","iopub.status.idle":"2024-11-26T12:42:10.340309Z","shell.execute_reply.started":"2024-11-26T12:42:10.333344Z","shell.execute_reply":"2024-11-26T12:42:10.339498Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"rabbitmq = RabbitMQ()\n\nresumes = []\n\ndef callbackfunct(ch, method, properties, body):\n    print(\"New resume\")\n    resumes.append(body.decode('utf-8'))\n\nrabbitmq.consume(queue_name='resume', callback=callbackfunct)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for resume in resumes:\n    print(\"Add new resume\")\n    rag.insert(resume)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:43:28.693844Z","iopub.execute_input":"2024-11-26T12:43:28.694637Z","iopub.status.idle":"2024-11-26T14:13:41.144616Z","shell.execute_reply.started":"2024-11-26T12:43:28.694601Z","shell.execute_reply":"2024-11-26T14:13:41.143361Z"}},"outputs":[{"name":"stdout","text":"Add new resume\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66173ea9a1e43e3be78a7dfaede470c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca48dee521a544678eb39e318088f34d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af4c2234954d4285b991627e3eefdb15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"365a343d93c745fd99ee01908913b5ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbc966637c543138d9fcb44d08a588b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957fdd35272a4d4c923a40db9a3639dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa32d2f0f08c40b8a2b9c4a6e29a49cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a31720b9b7f94fad967d07035cfb6c16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923302874585411596c4a9cecb6f9b18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4609ac4207e4026a6980576c661928f"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/132k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f905e9f7d0f4f5f8d0d44e2ed9de9d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a81b2a4274e4823a8bd07cb7536be8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d8297bfd09f4304a6ea3af962db982d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94319cd9553a4c549fa4cb9c731f7436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c659cc4d06346e3981140e7d37b4898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19538f56b3004dffb980acfe31547ce9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\n","output_type":"stream"},{"name":"stderr","text":"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","output_type":"stream"},{"name":"stdout","text":"KG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 9 entities(duplicated), 7 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 8 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 4 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 10 entities(duplicated), 8 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 6 entities(duplicated), 4 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 5 relations(duplicated)\nKG successfully indexed.\nAdd new resume\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"⠙ Processed 1 chunks, 7 entities(duplicated), 6 relations(duplicated)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resume \u001b[38;5;129;01min\u001b[39;00m resumes:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd new resume\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightrag/lightrag.py:195\u001b[0m, in \u001b[0;36mLightRAG.insert\u001b[0;34m(self, string_or_strings)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert\u001b[39m(\u001b[38;5;28mself\u001b[39m, string_or_strings):\n\u001b[1;32m    194\u001b[0m     loop \u001b[38;5;241m=\u001b[39m always_get_an_event_loop()\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mainsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring_or_strings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n","File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6}]}